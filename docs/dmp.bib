% This file was created with JabRef 2.10.
% Encoding: ISO8859_1


@Article{buchli11learning,
  title                    = {Learning Variable Impedance Control},
  author                   = {Jonas Buchli and Freek Stulp and Evangelos Theodorou and Stefan Schaal},
  journal                  = {International Journal of Robotics Research},
  year                     = {2011},
  number                   = {7},
  pages                    = {820-833},
  volume                   = {30},

  abstract                 = {One of the hallmarks of the performance, versatility, and robustness of biological motor control is the ability to adapt the impedance of the overall biomechanical system to different task requirements and stochastic disturbances. A transfer of this principle to robotics is desirable, for instance to enable robots to work robustly and safely in everyday human environments. It is, however, not trivial to derive variable impedance controllers for practical high degree-of-freedom (DOF) robotic tasks. In this contribution, we accomplish such variable impedance control with the reinforcement learning (RL) algorithm PI2 ({\bf P}olicy {\bf I}mprovement with {\bf P}ath {\bf I}ntegrals). PI2 is a model-free, sampling based learning method derived from first principles of stochastic optimal control. The PI2 algorithm requires no tuning of algorithmic parameters besides the exploration noise. The designer can thus fully focus on cost function design to specify the task. From the viewpoint of robotics, a particular useful property of PI2 is that it can scale to problems of many DOFs, so that reinforcement learning on real robotic systems becomes feasible. We sketch the PI2 algorithm and its theoretical properties, and how it is applied to gain scheduling for variable impedance control. We evaluate our approach by presenting results on several simulated and real robots. We consider tasks involving accurate tracking through via-points, and manipulation tasks requiring physical contact with the environment. In these tasks, the optimal strategy requires both tuning of a reference trajectory \emph{and} the impedance of the end-effector. The results show that we can use path integral based reinforcement learning not only for planning but also to derive variable gain feedback controllers in realistic scenarios. Thus, the power of variable impedance control is made available to a wide variety of robotic systems and practical applications.},
  bib2html_pubtype         = {Journal},
  bib2html_rescat          = {Reinforcement Learning of Variable Impedance Control},
  url                      = {http://ijr.sagepub.com/content/early/2011/03/31/0278364911402527}
}

@Article{ijspeert13dynamical,
  title                    = {{Dynamical Movement Primitives}: Learning Attractor Models for Motor Behaviors},
  author                   = {Ijspeert, A. and Nakanishi, J. and Pastor, P and Hoffmann, H. and Schaal, S.},
  journal                  = {Neural Computation},
  year                     = {2013},
  number                   = {2},
  pages                    = {328--373},
  volume                   = {25},

  file                     = {ijspeert13dynamical.pdf:unsorted/ijspeert13dynamical.pdf:PDF},
  key                      = {movement primitives, dynamic systems, rhythmic, discrete, perception-action coupling},
  url                      = {http://www-clmc.usc.edu/publications/I/ijspeert-NC2013.pdf}
}

@InProceedings{ijspeert02movement,
  title                    = {Movement imitation with nonlinear dynamical systems in humanoid robots},
  author                   = {A. J. Ijspeert and J. Nakanishi and S. Schaal},
  booktitle                = {Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)},
  year                     = {2002},

  file                     = {:/home/stulp/docs/bibliography/robotics/clmc-lab/Ijspeert, Nakanishi, Schaal - 2002 - Movement Imitation with Nonlinear Dynamical Systems in Humanoid Robots.pdf:PDF},
  keywords                 = {clmc lab}
}

@InProceedings{kalakrishnan11learning,
  title                    = {Learning force control policies for compliant manipulation},
  author                   = {Kalakrishnan, M. and Righetti, L. and Pastor, P. and Schaal, S.},
  booktitle                = {IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2011)},
  year                     = {2011},

  url                      = {http://www-clmc.usc.edu/publications/K/kalakrishnan-IROS2011}
}

@Article{kulvicius12joining,
  title                    = {Joining Movement Sequences: Modified Dynamic Movement Primitives for Robotics Applications Exemplified on Handwriting},
  author                   = {Tomas Kulvicius and KeJun Ning and Minija Tamosiunaite and Florentin W{\"o}rg{\"o}tter},
  journal                  = {IEEE Transactions on Robotics},
  year                     = {2012},
  number                   = {1},
  pages                    = {145-157},
  volume                   = {28},

  bibsource                = {DBLP, http://dblp.uni-trier.de},
  ee                       = {http://dx.doi.org/10.1109/TRO.2011.2163863},
  file                     = {kulvicius12joining.pdf:kulvicius12joining.pdf:PDF}
}

@Article{matsubara11learning,
  title                    = {Learning parametric dynamic movement primitives from multiple demonstrations},
  author                   = {Matsubara, T and Hyon, S and Morimoto, J},
  journal                  = {Neural Networks},
  year                     = {2011},
  number                   = {5},
  pages                    = {493-500},
  volume                   = {24},

  file                     = {matsubara11learning.pdf:unsorted/matsubara11learning.pdf:PDF}
}

@InProceedings{silva12learning,
  title                    = {Learning Parameterized Skills},
  author                   = {da Silva, Bruno and Konidaris, George and Barto, Andrew G.},
  booktitle                = {Proceedings of the 29th International Conference on Machine Learning (ICML-12)},
  year                     = {2012},

  address                  = {New York, NY, USA},
  editor                   = {John Langford and Joelle Pineau},
  month                    = {July},
  pages                    = {1679--1686},
  publisher                = {Omnipress},
  series                   = {ICML '12},

  file                     = {silva12learning.pdf:unsorted/silva12learning.pdf:PDF},
  isbn                     = {978-1-4503-1285-1},
  keywords                 = {manifolds, meta-parameters},
  location                 = {Edinburgh, Scotland, GB},
  review                   = {1) train K DMPs on K trajectories 2) learn D lower-dimensional surfaces (ISOMAP) 3) train a classifier that maps from T to which of the D surfaces 4) compute a regression from T to each policy parameter individually for each of the D surfaces (SVM)}
}

@InProceedings{stulp13learning,
  title                    = {Learning Compact Parameterized Skills with a Single Regression},
  author                   = {Freek Stulp and Gennaro Raiola and Antoine Hoarau and Serena Ivaldi and Olivier Sigaud},
  booktitle                = {IEEE-RAS International Conference on Humanoid Robots},
  year                     = {2013},

  bib2html_pubtype         = {Refereed Conference Paper},
  bib2html_rescat          = {Imitation Learning and Regression}
}

@comment{jabref-meta: selector_keywords:}

@comment{jabref-meta: selector_journal:}

@comment{jabref-meta: selector_publisher:}

@comment{jabref-meta: selector_author:}

